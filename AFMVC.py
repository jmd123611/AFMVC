import argparse           #qæ¶ˆèå®éªŒ å»æ‰å…¬å¹³æ€§æŸå¤±ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
import torch
import torch.nn.functional as F
import pandas as pd
from module1multi import MultiViewDFC  # å¯¼å…¥æ¨¡å‹
from utils import set_seed, target_distribution  # å·¥å…·å‡½æ•°
from eval import purity_score, fairness_evaluation, balance_score, ACC  # è¯„ä¼°æ–¹æ³•
from dataloader import load_data  # åŠ è½½æ•°æ®
from sklearn.metrics.cluster import normalized_mutual_info_score
import os
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.cluster import KMeans
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# ====== è§£æå‘½ä»¤è¡Œå‚æ•° ======
parser = argparse.ArgumentParser(description="Multi-View Fair Clustering Training")
parser.add_argument("--dataset", type=str, default="mfeat", help="é€‰æ‹©æ•°æ®é›†")
parser.add_argument("--hidden_dim", type=int, default=64, help="éšè—å±‚ç»´åº¦")
parser.add_argument("--batch_size", type=int, default=256, help="æ‰¹é‡å¤§å°")
parser.add_argument("--epochs_ae", type=int, default=100, help="è‡ªç¼–ç å™¨è®­ç»ƒè½®æ•°")
parser.add_argument("--epochs_dfc", type=int, default=1000, help="å…¬å¹³èšç±»è®­ç»ƒè½®æ•°")
parser.add_argument("--T_update", type=int, default=50, help="ä¼ªæ ‡ç­¾æ›´æ–°é—´éš”")
parser.add_argument("--learning_rate", type=float, default=5e-4, help="å­¦ä¹ ç‡")
parser.add_argument("--alpha_fair", type=float, default=0.01, help="å…¬å¹³æ€§æŸå¤±ç³»æ•°")
parser.add_argument("--alpha_recon", type=float, default=1, help="é‡å»ºæŸå¤±ç³»æ•°")
parser.add_argument("--alpha_entropy", type=float, default=0.1, help="äº¤å‰ç†µèšç±»æŸå¤±ç³»æ•°")
parser.add_argument("--hidden_size", type=int, default=32, help="å¯¹æŠ—ç½‘ç»œéšè—å±‚å¤§å°")
parser.add_argument("--max_iter", type=int, default=20000, help="å¯¹æŠ—ç½‘ç»œæœ€å¤§è¿­ä»£æ¬¡æ•°")
parser.add_argument("--lr_mult", type=float, default=0.5, help="å¯¹æŠ—ç½‘ç»œå­¦ä¹ ç‡å€ç‡")

args = parser.parse_args()


print(f"ğŸ“Œ ä½¿ç”¨æ•°æ®é›†: {args.dataset}")

# ====== è®­ç»ƒè¶…å‚æ•° ======
BATCH_SIZE = args.batch_size
NUM_EPOCHS_AE = args.epochs_ae
NUM_EPOCHS_DFC = args.epochs_dfc
LEARNING_RATE = args.learning_rate
ALPHA_FAIR = args.alpha_fair
ALPHA_ENTROPY = args.alpha_entropy
ALPHA_RECON = args.alpha_recon
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
HIDDEN_SIZE = args.hidden_size
MAX_ITER = args.max_iter
LR_MULT = args.lr_mult
T_update = args.T_update

# ====== åŠ è½½æ•°æ® ======
dataset, input_dims, num_views, data_size, num_clusters, train_loader = load_data(args.dataset)
num_sen=2
# è®¾ç½® `hidden_dim` é»˜è®¤å€¼
hidden_dim = args.hidden_dim if args.hidden_dim != -1 else int(sum(input_dims) / len(input_dims))

# åˆå§‹åŒ–æ¨¡å‹
model = MultiViewDFC(input_dims, hidden_dim, num_clusters, hidden_size=HIDDEN_SIZE,num_groups=num_sen, max_iter=MAX_ITER, lr_mult=LR_MULT).to(DEVICE)


# ====== æ•°æ®æ£€æŸ¥å‡½æ•° ======
def test_dataloader(dataset_name):
    """
    æ£€æŸ¥ DataLoader æ˜¯å¦æ­£ç¡®åŠ è½½æ•°æ®
    """
    dataset, dims, num_views, data_size, class_num, train_loader = load_data(dataset_name)

    for X_batch, G_batch, Y_batch in train_loader:
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›† '{dataset_name}'ï¼Œå…±æœ‰ {num_views} ä¸ªè§†å›¾")
        for i in range(num_views):
            print(f"è§†å›¾ {i+1} å½¢çŠ¶: {X_batch[i].shape}")
        print(f"å—ä¿æŠ¤å±æ€§ G å½¢çŠ¶: {G_batch.shape}")
        print(f"ç±»åˆ«æ ‡ç­¾ Y å½¢çŠ¶: {Y_batch.shape}")
        break  # åªæ‰“å°ä¸€ä¸ª batch


# è¿è¡Œæ•°æ®æ£€æŸ¥
test_dataloader(args.dataset)


# ====== è®­ç»ƒè‡ªç¼–ç å™¨ ======
def train_autoencoder(model, train_loader, num_epochs=NUM_EPOCHS_AE):
    """
    è®­ç»ƒ `AutoEncoder` ä»¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚
    """
    optimizer = optim.Adam(model.autoencoder.parameters(), lr=LEARNING_RATE)
    mse_loss_function = nn.MSELoss()

    print("=== å¼€å§‹è®­ç»ƒ AutoEncoder ===")
    for epoch in range(num_epochs):
        total_recon_loss = 0
        for batch in train_loader:
            X_batch, _, _ = batch
            X_batch = [x.to(DEVICE) for x in X_batch]

            optimizer.zero_grad()
            _, X_recon = model.autoencoder(X_batch)

            # è®¡ç®—é‡æ„æŸå¤±
            recon_loss = sum(mse_loss_function(x, x_recon) for x, x_recon in zip(X_batch, X_recon))

            # åå‘ä¼ æ’­
            recon_loss.backward()
            optimizer.step()

            total_recon_loss += recon_loss.item()

        avg_recon_loss = total_recon_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{num_epochs}] ")

    print("=== é¢„è®­ç»ƒ AutoEncoder ç»“æŸï¼Œå¼€å§‹ K-Means è®¡ç®—ä¼ªæ ‡ç­¾ ===")

def train_fair_clustering(model, train_loader, alpha_fair, alpha_entropy, num_epochs=NUM_EPOCHS_DFC,
                          ):
    print("å½“å‰å‚æ•°ç»„åˆï¼š", alpha_fair, alpha_entropy)

    def compute_kmeans_labels():
        """ä½¿ç”¨å½“å‰ `Z_fused` è®¡ç®— `con_labels` å¹¶æ›´æ–° `ClusterAssignment`"""
        encoded_views_per_encoder = [[] for _ in model.autoencoder.encoder.encoders]
        with torch.no_grad():
            for batch in train_loader:
                X_batch, _, _ = batch
                X_batch = [x.to(DEVICE) for x in X_batch]
                encoded_views = model.autoencoder.encoder(X_batch)
                for i in range(len(encoded_views)):
                    encoded_views_per_encoder[i].append(encoded_views[i].cpu().numpy())
        all_encoded_views = [np.vstack(encoded_views) for encoded_views in encoded_views_per_encoder]
        concatenated_Z = np.hstack(all_encoded_views)

        kmeans = KMeans(n_clusters=num_clusters, n_init=10)
        con_labels = torch.tensor(kmeans.fit_predict(concatenated_Z), dtype=torch.long, device=DEVICE)

        model.cluster_assignment.cluster_centers_fused.data = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32,
                                                                           device=DEVICE)
        return con_labels

    con_labels = compute_kmeans_labels()
    optimizer_E = optim.Adam([
        {"params": model.autoencoder.encoder.parameters(), "lr": LEARNING_RATE},
        {"params": model.autoencoder.decoder.parameters(), "lr": LEARNING_RATE},
        {"params": model.cluster_assignment.parameters(), "lr": LEARNING_RATE},
        {"params": model.adv_net.parameters(), "lr": LEARNING_RATE * model.adv_net.lr_mult}
    ])

    for epoch in range(num_epochs):
        if  epoch % T_update == 0:
            con_labels = compute_kmeans_labels()

        total_recon_loss = 0
        total_entropy_loss = 0
        total_fair_loss = 0
        total_samples = 0

        for batch in train_loader:
            X_batch, G_batch, _ = batch
            X_batch = [x.to(DEVICE) for x in X_batch]
            G_batch = G_batch.to(DEVICE).float().unsqueeze(1)

            optimizer_E.zero_grad()
            P_fused, P_views, Z_fused, adv_out, X_recon = model(X_batch, use_grl=True)

            ce_loss_function = nn.CrossEntropyLoss()
            recon_loss = sum(F.mse_loss(x, x_recon) for x, x_recon in zip(X_batch, X_recon))

            fair_loss = ce_loss_function(adv_out, G_batch.squeeze(1).long())

            con_labels_onehot = F.one_hot(con_labels, num_classes=num_clusters).float()
            entropy_loss = sum(
                F.kl_div(torch.log(P_view + 1e-8), con_labels_onehot, reduction='batchmean')
                for P_view in P_views
            )
            total_loss = ALPHA_RECON * recon_loss + alpha_fair * fair_loss + alpha_entropy * entropy_loss

            total_loss.backward()
            optimizer_E.step()
            total_recon_loss += recon_loss.item()
            total_fair_loss += fair_loss.item()
            total_entropy_loss += entropy_loss.item()
            total_samples += G_batch.size(0)
        print(
            f"Epoch {epoch + 1}/{num_epochs}  ")

    print("âœ… è®­ç»ƒå®Œæˆï¼")


# ====== è¯„ä¼°æ¨¡å‹ ======
def eval_model(model, train_loader):
    """
    è¯„ä¼°æ¨¡å‹çš„ Purityã€Fairness å’Œ Balance æŒ‡æ ‡
    """
    print("=== æ­£åœ¨è¯„ä¼°æ¨¡å‹ ===")
    all_preds, all_labels, all_groups = [], [], []

    with torch.no_grad():
        for batch in train_loader:
            X_batch, G_batch, Y_batch = batch
            X_batch = [x.to(DEVICE) for x in X_batch]

            # âœ… æ­£ç¡®è§£åŒ…
            _, P_views, Z_fused, _, _ = model(X_batch)

            # âœ… æŠŠ Z_fused ç§»åŠ¨åˆ° CPU
            Z_fused_cpu = Z_fused.detach().cpu().numpy()

            # âœ… åœ¨ Z_fused ä¸Šåº”ç”¨ K-means
            kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=0)
            preds = kmeans.fit_predict(Z_fused_cpu)

            # âœ… æ”¶é›†é¢„æµ‹ç»“æœ
            all_preds.append(preds)
            all_labels.append(Y_batch.cpu().numpy())
            all_groups.append(G_batch.cpu().numpy())

    balance= balance_score(np.concatenate(all_preds), np.concatenate(all_groups))
    acc = ACC(np.concatenate(all_labels), np.concatenate(all_preds))
    nmi = normalized_mutual_info_score(np.concatenate(all_labels), np.concatenate(all_preds))

    print(f" Balance: {balance:.4f} | ACC: {acc:.4f} | NMI: {nmi:.4f}")

    return balance,acc,nmi

# ====== è¿è¡Œè®­ç»ƒ ======
if __name__ == "__main__":
    for run in range(1):
        model = MultiViewDFC(input_dims, hidden_dim, num_clusters, hidden_size=HIDDEN_SIZE,num_groups=num_sen, max_iter=MAX_ITER,
                             lr_mult=LR_MULT).to(DEVICE)
        train_autoencoder(model, train_loader)
        train_fair_clustering(model, train_loader,alpha_fair=ALPHA_FAIR,alpha_entropy=ALPHA_ENTROPY)
        balance, acc, nmi = eval_model(model, train_loader)



